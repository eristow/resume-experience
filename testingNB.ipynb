{"cells":[{"cell_type":"markdown","metadata":{},"source":["##### Import Modules"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/miniconda/envs/mistral_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from tools import ACCESS_TOKEN"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Clear any cached CUDA memory\n","torch.cuda.empty_cache()\n","\n","# Device configuration\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Unused kwargs: ['bnb_4bit_use_double_qant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","/opt/miniconda/envs/mistral_env/lib/python3.11/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"name":"stdout","output_type":"stream","text":["...models loaded...\n"]}],"source":["# Quantization configuration\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_qant=True,\n",")\n","\n","# Model and tokenizer loading\n","BASE_MODEL = \"/home/tristow/mistral_env/models/mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=quant_config, token=ACCESS_TOKEN)\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=ACCESS_TOKEN)\n","\n","print(\"...models loaded...\")"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
