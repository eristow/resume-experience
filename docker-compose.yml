services:
  streamlit:
    container_name: streamlit

    image: resume_experience_streamlit

    init: true

    build:
      context: .
      dockerfile: streamlit.Dockerfile

    depends_on:
      ollama:
        condition: service_healthy

    ports:
      - '8501:8501'

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

    volumes:
      - ./src/models:/models
      - ./src:/src:ro

    environment:
      - WATCHDOG_TIMEOUT=10
      - STREAMLIT_SERVER_RUN_ON_SAVE=true
      - OLLAMA_BASE_URL=http://ollama:11434

    healthcheck:
      test: ['CMD-SHELL', 'curl localhost:8501/_stcore/health']
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 15s

  ollama:
    container_name: ollama

    image: resume_experience_ollama

    init: true

    build:
      context: .
      dockerfile: ollama.Dockerfile

    ports:
      - '11434:11434'

    environment:
      - OLLAMA_DEBUG=1

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
          memory: 4G
        limits:
          memory: 8G

    healthcheck:
      test: ['CMD-SHELL', 'curl localhost:11434']
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 15s
