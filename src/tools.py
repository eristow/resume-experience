"""
tools.py

This module provides various utility functions and tools for processing job descriptions and resumes. More specifically, it includes functions for analyzing the inputs, extracting text from files, and generating chat responses based on the analysis.
"""

import os
import traceback
import docx
import pdf2image
import pytesseract
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from pytesseract import Output
from custom_embeddings import CustomEmbeddings
from prompts import (
    ANALYSIS_QUESTION,
    ANALYSIS_PROMPT,
    CHAT_QUESTION,
    CHAT_PROMPT,
    passthrough,
)
import logging
import shutil

OCR_LANG = "eng"
logger = logging.getLogger(__name__)


def analyze_inputs(job_text, resume_text, ollama):
    """
    Analyzes the inputs by processing the job text and resume text using the Mistral model.

    Args:
        job_text (str): The text of the job description file.
        resume_text (str): The text of the resume file.
        ollama: The Ollama model used for generating the analysis response.

    Returns:
        str: The response generated by the Mistral model based on the analysis of the resume and job description.
            If the analysis fails, it returns "Failed to process the files."
        job_ad_retriever: The job ad retriever object.
        resume_retriever: The resume retriever object.
    """
    job_ad_vectorstore = None
    resume_vectorstore = None
    job_ad_retriever = None
    resume_retriever = None

    if job_text and resume_text:
        # Use the local Mistral model
        embeddings = CustomEmbeddings(model_name="./models/mistral")

        job_ad_vectorstore = process_text(job_text, embeddings)
        resume_vectorstore = process_text(resume_text, embeddings)
        logger.info(f"job_ad_vectorstore: {job_ad_vectorstore}")
        logger.info(f"resume_vectorstore: {resume_vectorstore}")

        if job_ad_vectorstore and resume_vectorstore:
            logger.info("Both vectorstores exist")
            job_ad_retriever = job_ad_vectorstore.as_retriever()
            resume_retriever = resume_vectorstore.as_retriever()
            logger.info("After creating retrievers")
            chain = (
                {
                    "resume_context": resume_retriever,
                    "job_ad_context": job_ad_retriever,
                    "question": lambda x: ANALYSIS_QUESTION,
                }
                | ANALYSIS_PROMPT
                | ollama
                | passthrough  # Simple output parsing
            )
            logger.info("After creating chain")
            response = chain.invoke("Analyze the resume based on the job description")
            logger.info("After invoking chain to generate response")
            return response, job_ad_retriever, resume_retriever

    logger.error("Failed to process the files.")
    return "Failed to process the files."


def extract_text(file_type, file, temp_dir):
    """
    Extracts text from a given file.

    Args:
        file_type (str): The type of file (either "job ad" or "resume").
        file (File): The uploaded file object.
        temp_dir (str): The temporary directory to save the uploaded file.

    Returns:
        str: The extracted text from the file, or None if no text is found.
    """

    if file_type not in ["job ad", "resume"]:
        logger.error(f"Invalid file type: {file_type}")
        return

    file_path = os.path.join(temp_dir, os.path.basename(file.name))
    shutil.copyfile(file.name, file_path)
    logger.info(f"Before extracting {file_type} text")
    text = extract_text_from_file(file, file_path)
    logger.info(f"After extracting {file_type} text")
    return text


def extract_text_from_file(uploaded_file, file_path):
    """
    Extracts text from a given file.

    Args:
        uploaded_file (File): The uploaded file object.
        file_path (str): The path to the uploaded file.

    Returns:
        str: The extracted text from the file, or None if no text is found.
    """
    file_extension = uploaded_file.name.split(".")[-1].lower()
    text = ""

    if file_extension == "pdf":
        text = extract_text_from_image(file_path)
    elif file_extension in ["doc", "docx"]:
        doc = docx.Document(file_path)
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"

    return None if not text else text


# Doesn't work if using uploaded_file. Only with file_path
def extract_text_from_image(file_path):
    """
    Extracts text from a PDF image using OCR (Optical Character Recognition).

    Args:
        file_path (str): The path to the PDF file.

    Returns:
        str: The extracted text from the PDF image.
    """

    text = ""
    images = pdf2image.convert_from_path(file_path)

    for image in images:
        ocr_dict = pytesseract.image_to_data(
            image, lang=OCR_LANG, output_type=Output.DICT
        )
        text += " ".join([word for word in ocr_dict["text"] if word])

    return text


def process_text(text, embeddings):
    """
    Process text by splitting it into chunks, and creating a vectorstore.

    Args:
        text (str): The text to be processed.
        embeddings: The embeddings object used for creating the vectorstore.

    Returns:
        vectorstore: The created vectorstore object.

    Raises:
        Exception: If an error occurs while creating the vectorstore.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
    logger.info(f"text_splitter: {text_splitter}")
    chunks = text_splitter.split_text(text)
    logger.info("chunks created from text_splitter")
    if chunks and embeddings:
        embeddings_list = embeddings.embed_documents(chunks)
        logger.info("embeddings_list created")

        if embeddings_list:
            logger.info(f"embeddings: {embeddings}")
            try:
                vectorstore = Chroma.from_texts(texts=chunks, embedding=embeddings)
                logger.info(f"vectorstore: {vectorstore}")
                return vectorstore
            except Exception as e:
                logger.error("An error occurred while creating the vectorstore:")
                traceback.print_exception(type(e), e, e.__traceback__)
                return None
    return None


def get_chat_response(user_input, job_ad_retriever, resume_retriever, ollama):
    """
    Retrieves a chat response based on the user input, resume retriever, and job ad retriever.

    Args:
        user_input (str): The user's input.
        resume_retriever: The resume retriever object.
        job_ad_retriever: The job ad retriever object.
        ollama: The Ollama model used for generating the chat response.

    Returns:
        str: The chat response.
    """
    # Rest of the code...
    logger.info(f"user_input: {user_input}")
    logger.info(f"resume_retriever: {resume_retriever}")
    logger.info(f"job_ad_retriever: {job_ad_retriever}")
    if not user_input or not job_ad_retriever or not resume_retriever:
        return None

    chain = (
        {
            "resume_context": resume_retriever,
            "job_ad_context": job_ad_retriever,
            "user_input": lambda x: user_input,
            "question": lambda x: CHAT_QUESTION,
        }
        | CHAT_PROMPT
        | ollama
        | passthrough  # Simple output parsing
    )

    response = chain.invoke(user_input)
    logger.info("Chat response created")
    return response
