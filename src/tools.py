"""
tools.py

This module provides various utility functions and tools for processing job descriptions and resumes. More specifically, it includes functions for analyzing the inputs, extracting text from files, and generating chat responses based on the analysis.
"""

import os
import traceback
import docx
import pdf2image
import pytesseract
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from pytesseract import Output
from custom_embeddings import CustomEmbeddings
from prompts import (
    ANALYSIS_QUESTION,
    ANALYSIS_PROMPT,
    CHAT_QUESTION,
    CHAT_PROMPT,
    passthrough,
)
import logging
import shutil

OCR_LANG = "eng"
logger = logging.getLogger(__name__)


def verify_input_size(job_text, resume_text):
    """
    Verify the input size and estimate total tokens including prompt overhead.

    Args:
        job_text (str): The job text to verify
        resume_text (str): The resume text to verify

    Returns:
        int: Estimated total tokens including prompt overhead

    Raises:
        ValueError: If the total estimated tokens would exceed context window
    """
    CONTEXT_WINDOW = 4096
    CHUNK_SIZE = 1024
    CHUNK_OVERLAP = 200
    TIMESTAMP_TOKENS = 15  # Approximate tokens for datetime

    ANALYSIS_PROMPT_TOKENS = len(ANALYSIS_QUESTION)
    CHAT_PROMPT_TOKENS = len(CHAT_QUESTION)

    estimated_text_tokens = len(job_text.split()) + len(resume_text.split())

    # Calculate chunks needed
    num_chunks = max(
        1, (estimated_text_tokens + CHUNK_OVERLAP) // (CHUNK_SIZE - CHUNK_OVERLAP)
    )
    total_chunk_tokens = num_chunks * CHUNK_SIZE

    prompt_overhead = ANALYSIS_PROMPT_TOKENS + TIMESTAMP_TOKENS
    total_tokens = total_chunk_tokens + prompt_overhead

    # Log the breakdown
    logger = logging.getLogger(__name__)
    logger.info(f"Text tokens: {estimated_text_tokens}")
    logger.info(f"Number of chunks: {num_chunks}")
    logger.info(f"Total chunk tokens: {total_chunk_tokens}")
    logger.info(f"Prompt overhead: {prompt_overhead}")
    logger.info(f"Total estimated tokens: {total_tokens}")

    # Verify against context window
    max_safe_tokens = CONTEXT_WINDOW - 100  # Leave buffer for response
    if total_tokens > max_safe_tokens:
        raise ValueError(
            f"Input would require approximately {total_tokens} tokens, "
            f"exceeding the {CONTEXT_WINDOW} token context window. "
            f"Please reduce the input size by about "
            f"{((total_tokens - max_safe_tokens) / total_tokens * 100):.1f}%"
        )

    return total_tokens


def analyze_inputs(job_text, resume_text, ollama):
    """
    Analyzes the inputs by processing the job text and resume text using the Mistral model.

    Args:
        job_text (str): The text of the job description file.
        resume_text (str): The text of the resume file.
        ollama: The Ollama model used for generating the analysis response.

    Returns:
        str: The response generated by the Mistral model based on the analysis of the resume and job description.
            If the analysis fails, it returns "Failed to process the files."
        job_ad_retriever: The job ad retriever object.
        resume_retriever: The resume retriever object.
    """
    job_ad_vectorstore = None
    resume_vectorstore = None
    job_ad_retriever = None
    resume_retriever = None

    if job_text and resume_text:
        try:
            total_tokens = verify_input_size(job_text, resume_text)

            logger.info(f"Total tokens (with overhead): {total_tokens}")
        except ValueError as e:
            logger.error(f"Input size verification failed: {e}")
            return (
                "Input too large. Please reduce the size of the job description or resume.",
                None,
                None,
            )

        # Use the local Mistral model
        embeddings = CustomEmbeddings(model_name="./models/mistral")

        job_ad_vectorstore = process_text(job_text, embeddings)
        resume_vectorstore = process_text(resume_text, embeddings)
        logger.info(f"job_ad_vectorstore: {job_ad_vectorstore}")
        logger.info(f"resume_vectorstore: {resume_vectorstore}")

        if job_ad_vectorstore and resume_vectorstore:
            logger.info("Both vectorstores exist")
            job_ad_retriever = job_ad_vectorstore.as_retriever()
            resume_retriever = resume_vectorstore.as_retriever()
            logger.info("After creating retrievers")
            chain = (
                {
                    "resume_context": resume_retriever,
                    "job_ad_context": job_ad_retriever,
                    "question": lambda x: ANALYSIS_QUESTION,
                }
                | ANALYSIS_PROMPT
                | ollama
                | passthrough  # Simple output parsing
            )
            logger.info("After creating chain")
            response = chain.invoke("Analyze the resume based on the job description")
            logger.info("After invoking chain to generate response")
            return response, job_ad_retriever, resume_retriever

    logger.error("Failed to process the files.")
    return "Failed to process the files."


def extract_text(file_type, file, temp_dir):
    """
    Extracts text from a given file.

    Args:
        file_type (str): The type of file (either "job ad" or "resume").
        file (File): The uploaded file object.
        temp_dir (str): The temporary directory to save the uploaded file.

    Returns:
        str: The extracted text from the file, or None if no text is found.
    """

    if file_type not in ["job ad", "resume"]:
        logger.error(f"Invalid file type: {file_type}")
        return

    logger.info(f"Extracting {file_type} text")
    file_path = os.path.join(temp_dir, os.path.basename(file.name))
    # This is for testing purposes...
    file_test = open("tests/test.pdf", "r")
    if type(file) is type(file_test):
        shutil.copyfile(file.name, file_path)
    else:
        with open(file_path, "wb") as temp_file:
            temp_file.write(file.getbuffer())

    logger.info(f"Before extracting {file_type} text")
    text = extract_text_from_file(file, file_path)
    logger.info(f"After extracting {file_type} text")
    return text


def extract_text_from_file(uploaded_file, file_path):
    """
    Extracts text from a given file.

    Args:
        uploaded_file (File): The uploaded file object.
        file_path (str): The path to the uploaded file.

    Returns:
        str: The extracted text from the file, or None if no text is found.
    """
    file_extension = uploaded_file.name.split(".")[-1].lower()
    text = ""

    if file_extension == "pdf":
        text = extract_text_from_image(file_path)
    elif file_extension in ["doc", "docx"]:
        doc = docx.Document(file_path)
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"

    return None if not text else text


# Doesn't work if using uploaded_file. Only with file_path
def extract_text_from_image(file_path):
    """
    Extracts text from a PDF image using OCR (Optical Character Recognition).

    Args:
        file_path (str): The path to the PDF file.

    Returns:
        str: The extracted text from the PDF image.
    """

    text = ""
    images = pdf2image.convert_from_path(file_path)

    for image in images:
        ocr_dict = pytesseract.image_to_data(
            image, lang=OCR_LANG, output_type=Output.DICT
        )
        text += " ".join([word for word in ocr_dict["text"] if word])

    return text


def process_text(text, embeddings):
    """
    Process text by splitting it into chunks, and creating a vectorstore.

    Args:
        text (str): The text to be processed.
        embeddings: The embeddings object used for creating the vectorstore.

    Returns:
        vectorstore: The created vectorstore object.

    Raises:
        Exception: If an error occurs while creating the vectorstore.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1024,
        chunk_overlap=200,
        length_function=lambda x: len(x.split()),
        separators=["\n\n", "\n", " ", ""],
    )
    logger.info(f"text_splitter: {text_splitter}")
    chunks = text_splitter.split_text(text)
    logger.info("chunks created from text_splitter")
    if chunks and embeddings:
        embeddings_list = embeddings.embed_documents(chunks)
        logger.info("embeddings_list created")

        if embeddings_list:
            logger.info(f"embeddings: {embeddings}")
            try:
                vectorstore = Chroma.from_texts(texts=chunks, embedding=embeddings)
                logger.info(f"vectorstore: {vectorstore}")
                return vectorstore
            except Exception as e:
                logger.error("An error occurred while creating the vectorstore:")
                traceback.print_exception(type(e), e, e.__traceback__)
                return None
    return None


def get_chat_response(user_input, job_ad_retriever, resume_retriever, ollama):
    """
    Retrieves a chat response based on the user input, resume retriever, and job ad retriever.

    Args:
        user_input (str): The user's input.
        resume_retriever: The resume retriever object.
        job_ad_retriever: The job ad retriever object.
        ollama: The Ollama model used for generating the chat response.

    Returns:
        str: The chat response.
    """
    logger.info(f"user_input: {user_input}")
    logger.info(f"resume_retriever: {resume_retriever}")
    logger.info(f"job_ad_retriever: {job_ad_retriever}")
    if not user_input or not job_ad_retriever or not resume_retriever:
        return None

    chain = (
        {
            "resume_context": resume_retriever,
            "job_ad_context": job_ad_retriever,
            "user_input": lambda x: user_input,
            "question": lambda x: CHAT_QUESTION,
        }
        | CHAT_PROMPT
        | ollama
        | passthrough  # Simple output parsing
    )

    response = chain.invoke(user_input)
    logger.info("Chat response created")
    return response
